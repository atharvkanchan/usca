{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "68fd38b9",
      "metadata": {
        "id": "68fd38b9"
      },
      "source": [
        "# Universal Supply Chain ML Notebook\n",
        "\n",
        "*Generated: 2025-11-12T12:12:29.324189 UTC*\n",
        "\n",
        "This notebook automatically ingests **any CSV dataset**, detects a sensible target variable, preprocesses the data, trains a model (tree-based or neural net depending on size), evaluates it, and saves the model. Designed for Colab / Jupyter.\n",
        "\n",
        "**How to use:** Upload a CSV via the file uploader cell, or mount Google Drive and set `DATA_PATH`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "77c35490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77c35490",
        "outputId": "a75b86f4-b282-46eb-beb7-ed638a1d9b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports done.\n"
          ]
        }
      ],
      "source": [
        "# Install common packages (uncomment if needed in Colab)\n",
        "# !pip install -q scikit-learn pandas matplotlib seaborn xgboost tensorflow joblib\n",
        "\n",
        "import os, sys, math, random, warnings\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "warnings.filterwarnings('ignore')\n",
        "print('Imports done.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30317fb3",
      "metadata": {
        "id": "30317fb3"
      },
      "outputs": [],
      "source": [
        "# ====== Dataset upload / path ======\n",
        "# Option A: Use Colab file upload (works in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        DATA_PATH = list(uploaded.keys())[0]\n",
        "        print('Uploaded:', DATA_PATH)\n",
        "except Exception:\n",
        "    DATA_PATH = None\n",
        "\n",
        "# Option B: If running locally, set DATA_PATH to your CSV\n",
        "if DATA_PATH is None:\n",
        "    # Try common filenames in working dir\n",
        "    candidates = [p for p in os.listdir('.') if p.lower().endswith('.csv')]\n",
        "    if candidates:\n",
        "        print('Found CSVs in working dir:', candidates)\n",
        "        DATA_PATH = candidates[0]\n",
        "    else:\n",
        "        print('No uploaded CSV detected. Please set DATA_PATH = \"your_file.csv\" and run this cell again.')\n",
        "\n",
        "DATA_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab35079",
      "metadata": {
        "id": "5ab35079"
      },
      "outputs": [],
      "source": [
        "# ====== Load dataset ======\n",
        "assert DATA_PATH is not None, 'Set DATA_PATH to your CSV file path.'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print('Loaded dataset with shape:', df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0158d5a9",
      "metadata": {
        "id": "0158d5a9"
      },
      "outputs": [],
      "source": [
        "# ====== Inspect and suggest target column ======\n",
        "print('Columns:')\n",
        "for i,c in enumerate(df.columns):\n",
        "    print(i+1, c)\n",
        "\n",
        "# Heuristics to guess target columns (numeric columns commonly used)\n",
        "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
        "print('\\nNumeric columns candidates (for regression targets):', num_cols)\n",
        "print('Categorical columns candidates (for classification targets):', cat_cols)\n",
        "\n",
        "# Common target name hints\n",
        "hints = ['sales','demand','quantity','units','price','cost','lead_time','delay','on_time','profit','revenue','consumption']\n",
        "possible_targets = [c for c in df.columns if any(h in c.lower() for h in hints)]\n",
        "print('\\nAuto-detected target candidates by name:', possible_targets)\n",
        "\n",
        "# Choose target: prefer numeric hint, else pick largest variance numeric column\n",
        "TARGET = None\n",
        "if possible_targets:\n",
        "    # pick numeric among them if possible\n",
        "    for t in possible_targets:\n",
        "        if t in num_cols:\n",
        "            TARGET = t\n",
        "            break\n",
        "if TARGET is None and num_cols:\n",
        "    # choose numeric column with highest variance (excluding id-like columns)\n",
        "    variances = {c: df[c].var() for c in num_cols}\n",
        "    sorted_vars = sorted(variances.items(), key=lambda x: x[1], reverse=True)\n",
        "    TARGET = sorted_vars[0][0]\n",
        "\n",
        "print('Suggested TARGET:', TARGET)\n",
        "\n",
        "# If you want to override, set TARGET = 'your_column' and re-run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d97bc62",
      "metadata": {
        "id": "1d97bc62"
      },
      "outputs": [],
      "source": [
        "# ====== Basic cleaning & preprocessing ======\n",
        "# 1) Drop columns that are mostly unique ids or have too many missing values\n",
        "THRESH_MISSING_RATIO = 0.6\n",
        "missing_ratio = df.isna().mean()\n",
        "drop_cols = missing_ratio[missing_ratio > THRESH_MISSING_RATIO].index.tolist()\n",
        "print('Dropping high-missing columns:', drop_cols)\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "# 2) Parse dates if any\n",
        "for c in df.columns:\n",
        "    if df[c].dtype == 'object':\n",
        "        try:\n",
        "            parsed = pd.to_datetime(df[c], errors='coerce')\n",
        "            non_na = parsed.notna().sum()\n",
        "            if non_na > len(df)*0.3:\n",
        "                df[c+'_dt'] = parsed\n",
        "                print('Parsed date column:', c, '->', c+'_dt')\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# 3) Add date features from any *_dt columns\n",
        "date_cols = [c for c in df.columns if c.endswith('_dt')]\n",
        "for c in date_cols:\n",
        "    df[c+'_year'] = df[c].dt.year\n",
        "    df[c+'_month'] = df[c].dt.month\n",
        "    df[c+'_day'] = df[c].dt.day\n",
        "    df[c+'_weekday'] = df[c].dt.weekday\n",
        "\n",
        "# 4) Remove obvious ID columns (all unique values)\n",
        "for c in df.columns:\n",
        "    if df[c].nunique() == len(df):\n",
        "        print('Dropping unique-id column:', c)\n",
        "        df = df.drop(columns=[c])\n",
        "\n",
        "print('Post-clean shape:', df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2af9699",
      "metadata": {
        "id": "f2af9699"
      },
      "outputs": [],
      "source": [
        "# ====== Prepare X and y =====n\n",
        "assert 'TARGET' in globals() and TARGET is not None, 'Set TARGET variable first.'\n",
        "y = df[TARGET]\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# Keep numeric and categorical separately\n",
        "num_features = X.select_dtypes(include=['number']).columns.tolist()\n",
        "cat_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "print('Numeric features:', num_features[:10])\n",
        "print('Categorical features:', cat_features[:10])\n",
        "\n",
        "# Simple frequency filter for high-cardinality categoricals\n",
        "HIGH_CARD_THRESH = 100\n",
        "cat_features = [c for c in cat_features if X[c].nunique() <= HIGH_CARD_THRESH]\n",
        "print('Filtered categoricals (<=100 unique):', cat_features)\n",
        "\n",
        "# Fill small missing values for numeric and categorical\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Pipelines\n",
        "num_pipeline = Pipeline([('imputer', num_imputer), ('scaler', StandardScaler())])\n",
        "cat_pipeline = Pipeline([('imputer', cat_imputer)])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_pipeline, num_features),\n",
        "    ('cat', cat_pipeline, cat_features)\n",
        "], remainder='drop')\n",
        "\n",
        "print('Preprocessor ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869f0013",
      "metadata": {
        "id": "869f0013"
      },
      "outputs": [],
      "source": [
        "# ====== Train/Test split ======\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
        "\n",
        "# Fit preprocessor on train\n",
        "preprocessor.fit(X_train)\n",
        "X_train_trans = preprocessor.transform(X_train)\n",
        "X_test_trans = preprocessor.transform(X_test)\n",
        "\n",
        "# Decide model type: regression vs classification\n",
        "is_regression = pd.api.types.is_numeric_dtype(y_train)\n",
        "print('Problem type inferred:', 'Regression' if is_regression else 'Classification')\n",
        "\n",
        "if is_regression:\n",
        "    model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train_trans, y_train)\n",
        "    preds = model.predict(X_test_trans)\n",
        "    print('MSE:', mean_squared_error(y_test, preds))\n",
        "    print('R2:', r2_score(y_test, preds))\n",
        "else:\n",
        "    model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train_trans, y_train)\n",
        "    preds = model.predict(X_test_trans)\n",
        "    print('Accuracy:', accuracy_score(y_test, preds))\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "# Save pipeline (preprocessor + model)\n",
        "artifact_path = 'universal_supply_chain_model.joblib'\n",
        "joblib.dump({'preprocessor': preprocessor, 'model': model}, artifact_path)\n",
        "print('Saved model pipeline to', artifact_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97a5810",
      "metadata": {
        "id": "c97a5810"
      },
      "outputs": [],
      "source": [
        "# ====== Quick evaluation plots ======\n",
        "try:\n",
        "    if is_regression:\n",
        "        plt.figure(figsize=(6,5))\n",
        "        plt.scatter(y_test, preds, alpha=0.5)\n",
        "        plt.xlabel('True')\n",
        "        plt.ylabel('Predicted')\n",
        "        plt.title('True vs Predicted')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        residuals = y_test - preds\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.histplot(residuals, kde=True)\n",
        "        plt.title('Residuals')\n",
        "        plt.show()\n",
        "    else:\n",
        "        # Confusion matrix\n",
        "        from sklearn.metrics import ConfusionMatrixDisplay\n",
        "        ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print('Plotting failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1dfae22",
      "metadata": {
        "id": "c1dfae22"
      },
      "outputs": [],
      "source": [
        "# ====== Predict on new rows (example) ======\n",
        "# Provide a small sample or load another CSV to predict on\n",
        "PRED_PATH = None  # set to 'new_data.csv' to load\n",
        "if PRED_PATH:\n",
        "    new_df = pd.read_csv(PRED_PATH)\n",
        "    pp = joblib.load(artifact_path)\n",
        "    X_new = new_df[X.columns]  # ensure same columns\n",
        "    X_new_trans = pp['preprocessor'].transform(X_new)\n",
        "    new_preds = pp['model'].predict(X_new_trans)\n",
        "    print('Predictions for new data:', new_preds[:10])\n",
        "else:\n",
        "    print('No PRED_PATH provided. To predict, set PRED_PATH to a new CSV file path and re-run.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e9f479",
      "metadata": {
        "id": "27e9f479"
      },
      "outputs": [],
      "source": [
        "# ====== (Colab only) Download the trained model\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(artifact_path)\n",
        "except Exception:\n",
        "    print('If running locally, the artifact is saved at', artifact_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}